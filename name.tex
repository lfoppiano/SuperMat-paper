% In this section we are discussing the process of annotation, in particular: 
%   1. how did we find the right balance of annotation, 
%   2. results in term of IAA

%After creating the logical model, selecting which relevant information to collect and subsequently define the labels (or tag-set), we established the annotations guidelines and the correction / cross-validation process. 
% In this article we explore the details of our corpus and the method and approach used for its construction. 

% In Text and Data Mining (TDM), the employment of Machine Learning is currently providing better accuracy and precision, more tolerance to noise, and flexibility in recognising entities that have not been seen before. These advantages are not coming for free, because in supervised learning, the system requires a certain amount of examples for training, test and validation. 

% The process of annotation of new training data is very expensive: 
% \begin{enumerate}
%     \item greedy in term of time and resources
%     \item the system might require a lot of data before showing performance accuracy improvements
%     \item is a tedious and frustrating for annotators (usually domain experts,  feeling overly-skilled, thus less motivated)
%     \item throughput and precision are inversely proportional
% \end{enumerate}

% In interdisciplinary projects, the process of annotation is always a collaborative work between ML engineers and domain experts. While the former are responsible to deal with the practical complexity of the problem, the latter steer over the content importance and the features requirements. 


