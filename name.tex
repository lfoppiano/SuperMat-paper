% In this section we are discussing the process of annotation, in particular: 
%   1. how did we find the right balance of annotation, 
%   2. results in term of IAA

%After creating the logical model, selecting which relevant information to collect and subsequently define the labels (or tag-set), we established the annotations guidelines and the correction / cross-validation process. 
% In this article we explore the details of our corpus and the method and approach used for its construction. 

% In Text and Data Mining (TDM), the employment of Machine Learning is currently providing better accuracy and precision, more tolerance to noise, and flexibility in recognising entities that have not been seen before. These advantages are not coming for free, because in supervised learning, the system requires a certain amount of examples for training, test and validation. 

% The process of annotation of new training data is very expensive: 
% \begin{enumerate}
%     \item greedy in term of time and resources
%     \item the system might require a lot of data before showing performance accuracy improvements
%     \item is a tedious and frustrating for annotators (usually domain experts,  feeling overly-skilled, thus less motivated)
%     \item throughput and precision are inversely proportional
% \end{enumerate}

% In interdisciplinary projects, the process of annotation is always a collaborative work between ML engineers and domain experts. While the former are responsible to deal with the practical complexity of the problem, the latter steer over the content importance and the features requirements. 

After the initial introduction with the domain experts, we, the ML engineers and data scientists, have spent time exploring the domain and attempting to define an annotation schema based on our understanding together with the knowledge of the technologies in our hands. We striven to reach a common understanding and "internally" agreed among us, before requesting the validation from the domain experts. 
In this way we a) asses our knowledge of the domain, b) increase the probabilities to spot problems related to the data by working with it in early stages, c) develop a proactive thinking of possible shortcut or additional constraints and, last but not least, we are ready to justify any additional constraints or decision to domain experts. 

% This section should describe how we are doing and which problems we are facing
We have selected two Open Access papers deposited on Arxiv (Creative Commons) and distributed among us (3 people). We have then pre-annotated them using the current prototype and corrected using 4 labels: <supercon>, <tc>, <propertyValue> and <substitution> to identify respectively superconductor materials, critical temperature expression, values and variables substitution combination. 

We have performed 3 iterative cycles of annotations, at the end of which, after calculating our Inter Annotation Agreement (IAA) we reviewed thoughtfully while updating the annotation guidelines, a "living" document describing how annotate each labels.

In Table \ref{table:summary-iaa} we summarise the IAA in these iterations. Looking at these data, we can see that <substitution> was the more unclear label, as had very low agreement until the 3rd iteration. This is due to the fact that it was appearing in many different form. On the other hand any mention of critical temperature (label <tc>) was more clear (reach 85\% agreement at iteration 2). 

\begin{table}[h!]
    \centering
    \begin{tabular}{ | c | c| c| } 
    \hline
        Iteration \# & IAA & IAA by label  \\ [0.5ex] 
    \hline\hline
        1  & 0.45
        &\begin{tabular}{  c | c  } 
            supercon & 0.45\\ 
            tc & 0.56\\
            propertyValue & 0.50\\
            substitution & 0.21\\
        \end{tabular}    
        \\ 
    \hline
        2 & 0.65
        &\begin{tabular}{  c |  c  } 
            supercon & 0.75\\ 
            tc & 0.85\\
            propertyValue & 0.85\\
            substitution & 0.39 \\
        \end{tabular}          
        \\ 
    \hline
        3 & 0.89
        & \begin{tabular}{  c | c  } 
            supercon & 0.89\\ 
            tc & 0.91\\
            propertyValue & 0.88\\
            substitution & 0.94\\
        \end{tabular}       
        
        \\ 
    \hline
    \end{tabular}
    \caption{Summary of the IAA for each of the three cycles of annotation. Together with the average annotation agreement, we publish the agreement by label.}
    \label{table:summary-iaa}
\end{table}
